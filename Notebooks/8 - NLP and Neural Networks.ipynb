{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "df = pd.read_csv('first_week_oct_2015_comments_by_top_400_with_scores_and_features_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent</th>\n",
       "      <th>by</th>\n",
       "      <th>time</th>\n",
       "      <th>hour_posted</th>\n",
       "      <th>text</th>\n",
       "      <th>dead</th>\n",
       "      <th>ranking</th>\n",
       "      <th>text_len</th>\n",
       "      <th>pct_caps</th>\n",
       "      <th>tags_per_char</th>\n",
       "      <th>papi_toxicity</th>\n",
       "      <th>v_neg</th>\n",
       "      <th>v_neu</th>\n",
       "      <th>v_pos</th>\n",
       "      <th>v_compound</th>\n",
       "      <th>tb_polarity</th>\n",
       "      <th>tb_subjectivity</th>\n",
       "      <th>tb_nb_prob_neg</th>\n",
       "      <th>pc_prob_offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10331981</td>\n",
       "      <td>10331895</td>\n",
       "      <td>debacle</td>\n",
       "      <td>2015-10-05 14:24:42+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>US is not really scared by BRICS at all. They'...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.61875</td>\n",
       "      <td>0.080001</td>\n",
       "      <td>0.238871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10343811</td>\n",
       "      <td>10343761</td>\n",
       "      <td>sarciszewski</td>\n",
       "      <td>2015-10-07 02:13:15+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>I wasn't really trying to argue, they said the...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048637</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3947</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.593498</td>\n",
       "      <td>0.050161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10331538</td>\n",
       "      <td>10331008</td>\n",
       "      <td>debacle</td>\n",
       "      <td>2015-10-05 13:08:10+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>The examples on the homepage kind of underscor...</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>88</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044777</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.336201</td>\n",
       "      <td>0.098511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10340097</td>\n",
       "      <td>10339965</td>\n",
       "      <td>debacle</td>\n",
       "      <td>2015-10-06 16:33:06+00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>No mention of a critical aspect of a service l...</td>\n",
       "      <td>False</td>\n",
       "      <td>22</td>\n",
       "      <td>99</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035335</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>0.352414</td>\n",
       "      <td>0.056323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10338552</td>\n",
       "      <td>10337763</td>\n",
       "      <td>debacle</td>\n",
       "      <td>2015-10-06 13:06:26+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>I think some of these points are gross exagger...</td>\n",
       "      <td>False</td>\n",
       "      <td>38</td>\n",
       "      <td>868</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>0.232577</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.8233</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.001499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    parent            by                       time  hour_posted  \\\n",
       "0  10331981  10331895       debacle  2015-10-05 14:24:42+00:00           14   \n",
       "1  10343811  10343761  sarciszewski  2015-10-07 02:13:15+00:00            2   \n",
       "2  10331538  10331008       debacle  2015-10-05 13:08:10+00:00           13   \n",
       "3  10340097  10339965       debacle  2015-10-06 16:33:06+00:00           16   \n",
       "4  10338552  10337763       debacle  2015-10-06 13:06:26+00:00           13   \n",
       "\n",
       "                                                text   dead  ranking  \\\n",
       "0  US is not really scared by BRICS at all. They'...  False        0   \n",
       "1  I wasn't really trying to argue, they said the...  False        0   \n",
       "2  The examples on the homepage kind of underscor...  False       12   \n",
       "3  No mention of a critical aspect of a service l...  False       22   \n",
       "4  I think some of these points are gross exagger...  False       38   \n",
       "\n",
       "   text_len  pct_caps  tags_per_char  papi_toxicity  v_neg  v_neu  v_pos  \\\n",
       "0       146  0.068493       0.000000       0.100881  0.000  0.744  0.256   \n",
       "1        76  0.013158       0.000000       0.048637  0.195  0.805  0.000   \n",
       "2        88  0.034091       0.000000       0.044777  0.000  0.864  0.136   \n",
       "3        99  0.010101       0.000000       0.035335  0.214  0.667  0.119   \n",
       "4       868  0.013825       0.009217       0.232577  0.081  0.902  0.017   \n",
       "\n",
       "   v_compound  tb_polarity  tb_subjectivity  tb_nb_prob_neg  pc_prob_offensive  \n",
       "0      0.7859     0.156250          0.61875        0.080001           0.238871  \n",
       "1     -0.3947     0.200000          0.20000        0.593498           0.050161  \n",
       "2      0.2975     0.137500          0.50000        0.336201           0.098511  \n",
       "3     -0.2500     0.000000          0.80000        0.352414           0.056323  \n",
       "4     -0.8233     0.076667          0.26000        0.000790           0.001499  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'parent', 'by', 'time', 'hour_posted', 'text', 'dead', 'ranking',\n",
       "       'text_len', 'pct_caps', 'tags_per_char', 'papi_toxicity', 'v_neg',\n",
       "       'v_neu', 'v_pos', 'v_compound', 'tb_polarity', 'tb_subjectivity',\n",
       "       'tb_nb_prob_neg', 'pc_prob_offensive'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review column list.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target column.\n",
    "y = df['papi_toxicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test subsets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], \n",
    "                                                    y, \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert comment text to lowercase, remove punctuation, and trim \n",
    "# whitespace.\n",
    "simple_text_train = X_train.str.lower()\n",
    "simple_text_train = simple_text_train.str.replace('[{}]'.format(string.punctuation), '')\n",
    "simple_text_train = simple_text_train.str.replace('\\s+', ' ', regex=True)\n",
    "simple_text_train = simple_text_train.str.strip()\n",
    "\n",
    "simple_text_test = X_test.str.lower()\n",
    "simple_text_test = simple_text_test.str.replace('[{}]'.format(string.punctuation), '')\n",
    "simple_text_test = simple_text_test.str.replace('\\s+', ' ', regex=True)\n",
    "simple_text_test = simple_text_test.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without the safe harbor agreement you can no longer avoid eu privacy regulations by storing the data in the us maybe im missing something here my understanding is that the safe harbour agreement wasnt a mechanism for us companies to avoid eu data protection regulations it was a certification that they did comply with eu data protection particularly in situations where that data was transmitted outside the eu now its gone eu customer data held by us companies will be governed by national data protection laws instead so may end up having to be stored within the eu the us privacy regulations are no longer considered compatible with the eu privacy regulations i dont think they ever were which is why the safe harbour needed to exist in the first place \n",
      "\n",
      "imagine how much a interlispd smalltalk mesacedar workstation would have cost in the 70s versus a plain pdp11 eh if theyd put altos in serial production instead of small batches at a time adding up to 2000 units it wouldnt have been vastly more expensive there was nothing both exotic and wildly expensive about them compared to contemporary pdp11s a graphics console the extras were a mouse chord keyboard network adapter and more memory per person than normal for a pdp11 \n",
      "\n",
      "thanks for the links as the current defacto maintainer of ppi the answer is rather simple pt predates ppi by almost two years httpsmetacpanorgsourceshancockperltidy20021130chan httpsmetacpanorgsourceadamkppi01changes and for the longest time perl was considered unparsable primarily due to the two features of function parens being optional and the argumentslurpiness of function calls being unknowable without introspecting the function reference that ends up being the final one at runtime in the most famous example this can lead to a after a function call being considered either the division operator or the start of a regex with both interpretations resulting in valid perl code it took a while for anyone to come up with a schema in which perl could be parsed while also being roundtrippable it took ppi a while to get there and be stable and meanwhile pt had already become stable itself \n",
      "\n",
      "surely you can give a rough idea \n",
      "\n",
      "disclaimer im a browser engine developer not a frontend or fullstack web developer that is why you have a sane view on the wed dev trenches more is better we are all insane ppl who think adding react to a page will make it spiffier \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check results.\n",
    "for text in simple_text_train.head():\n",
    "    print(text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text.\n",
    "tokens_train = simple_text_train.apply(nlp.tokenizer)\n",
    "tokens_test = simple_text_test.apply(nlp.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize; remove stop words.\n",
    "lemmas_train = tokens_train.apply(lambda x: [token.lemma_ for token in x if not token.is_stop])\n",
    "lemmas_test = tokens_test.apply(lambda x: [token.lemma_ for token in x if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lemmatize; leave stop words. Picks up \"you\" and \"your\" as potentially \"toxic\" words.\n",
    "# lemmas_train = tokens_train.apply(lambda x: [token.lemma_ for token in x])\n",
    "# lemmas_test = tokens_test.apply(lambda x: [token.lemma_ for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1362    [safe, harbor, agreement, long, avoid, eu, pri...\n",
       "7304    [imagine, interlispd, smalltalk, mesacedar, wo...\n",
       "4237    [thank, link, current, defacto, maintainer, pp...\n",
       "3680                                [surely, rough, idea]\n",
       "4780    [disclaimer, be, browser, engine, developer, f...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results.\n",
    "lemmas_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>101</th>\n",
       "      <th>10k</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3588 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100  1000  101  10k   11   12  120   13   14  ...  yes  yesterday  \\\n",
       "0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "1  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "2  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "3  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "4  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "\n",
       "   yield  york  young  youth  youtube  zero  zillow  zone  \n",
       "0    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "1    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "2    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "3    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "4    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "\n",
       "[5 rows x 3588 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate vectorizer object.\n",
    "tfidf = TfidfVectorizer(min_df=0.001, max_df=0.999)\n",
    "\n",
    "# Create a vocabulary and get word counts per document.\n",
    "dtm_train = tfidf.fit_transform(lemmas_train.astype(str))\n",
    "dtm_test = tfidf.transform(lemmas_test.astype(str))\n",
    "\n",
    "# Get feature names to use as dataframe column headers.\n",
    "dtm_train_orig = pd.DataFrame(dtm_train.todense(), columns=tfidf.get_feature_names())\n",
    "dtm_test_orig = pd.DataFrame(dtm_test.todense(), columns=tfidf.get_feature_names())\n",
    "\n",
    "# Copy document term matrices.\n",
    "dtm_train = dtm_train_orig.copy()\n",
    "dtm_test = dtm_test_orig.copy()\n",
    "\n",
    "# Preview feature matrix.\n",
    "dtm_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated feature (word) selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PerspectiveAPI toxicity scores to binary class labels.\n",
    "y_train_binary = (y_train > 0.7).astype(int)\n",
    "y_test_binary = (y_test > 0.7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7893,   83], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check target distribution.\n",
    "np.bincount(y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(docs):\n",
    "    \"\"\"\n",
    "    Helper function for feature selection.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    appears_in = Counter()\n",
    "    \n",
    "    total_docs = len(docs)\n",
    "\n",
    "    for doc in docs:\n",
    "        word_counts.update(doc)\n",
    "        appears_in.update(set(doc))\n",
    "\n",
    "    temp = zip(word_counts.keys(), word_counts.values())\n",
    "      \n",
    "    wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
    "\n",
    "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
    "    total = wc['count'].sum()\n",
    "\n",
    "    wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
    "        \n",
    "    wc = wc.sort_values(by='rank')\n",
    "    wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
    "\n",
    "    t2 = zip(appears_in.keys(), appears_in.values())\n",
    "    ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
    "    wc = ac.merge(wc, on='word')\n",
    "\n",
    "    wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
    "        \n",
    "    return wc.sort_values(by='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>appears_in</th>\n",
       "      <th>count</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_total</th>\n",
       "      <th>cul_pct_total</th>\n",
       "      <th>appears_in_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>not</td>\n",
       "      <td>35</td>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.421687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>fuck</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.029548</td>\n",
       "      <td>0.253012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>people</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.039679</td>\n",
       "      <td>0.265060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>shit</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.049388</td>\n",
       "      <td>0.253012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>like</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.156627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>s</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>0.066695</td>\n",
       "      <td>0.180723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>amazon</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.007176</td>\n",
       "      <td>0.073871</td>\n",
       "      <td>0.036145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>time</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.080625</td>\n",
       "      <td>0.156627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>work</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.087379</td>\n",
       "      <td>0.108434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>have</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.093710</td>\n",
       "      <td>0.132530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  appears_in  count  rank  pct_total  cul_pct_total  appears_in_pct\n",
       "12      not          35     46   1.0   0.019417       0.019417        0.421687\n",
       "17     fuck          21     24   2.0   0.010131       0.029548        0.253012\n",
       "46   people          22     24   3.0   0.010131       0.039679        0.265060\n",
       "51     shit          21     23   4.0   0.009709       0.049388        0.253012\n",
       "177    like          13     21   5.0   0.008864       0.058252        0.156627\n",
       "54        s          15     20   6.0   0.008442       0.066695        0.180723\n",
       "886  amazon           3     17   7.0   0.007176       0.073871        0.036145\n",
       "43     time          13     16   8.0   0.006754       0.080625        0.156627\n",
       "204    work           9     16   9.0   0.006754       0.087379        0.108434\n",
       "100    have          11     15  10.0   0.006332       0.093710        0.132530"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most common words in toxic comments.\n",
    "toxic_wc_train = count(lemmas_train[y_train_binary.astype(bool)])\n",
    "toxic_wc_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>appears_in</th>\n",
       "      <th>count</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_total</th>\n",
       "      <th>cul_pct_total</th>\n",
       "      <th>appears_in_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>not</td>\n",
       "      <td>2984</td>\n",
       "      <td>4872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>0.378057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>people</td>\n",
       "      <td>1485</td>\n",
       "      <td>2272</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>0.188141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>like</td>\n",
       "      <td>1439</td>\n",
       "      <td>1900</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.182313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>s</td>\n",
       "      <td>1246</td>\n",
       "      <td>1590</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.042952</td>\n",
       "      <td>0.157861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>think</td>\n",
       "      <td>1196</td>\n",
       "      <td>1508</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>0.049043</td>\n",
       "      <td>0.151527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>thing</td>\n",
       "      <td>985</td>\n",
       "      <td>1287</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.124794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>work</td>\n",
       "      <td>887</td>\n",
       "      <td>1245</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.059270</td>\n",
       "      <td>0.112378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>time</td>\n",
       "      <td>880</td>\n",
       "      <td>1147</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.111491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>use</td>\n",
       "      <td>826</td>\n",
       "      <td>1088</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>0.068297</td>\n",
       "      <td>0.104650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>have</td>\n",
       "      <td>887</td>\n",
       "      <td>1078</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.072651</td>\n",
       "      <td>0.112378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  appears_in  count  rank  pct_total  cul_pct_total  appears_in_pct\n",
       "21      not        2984   4872   1.0   0.019678       0.019678        0.378057\n",
       "223  people        1485   2272   2.0   0.009177       0.028855        0.188141\n",
       "382    like        1439   1900   3.0   0.007674       0.036530        0.182313\n",
       "191       s        1246   1590   4.0   0.006422       0.042952        0.157861\n",
       "24    think        1196   1508   5.0   0.006091       0.049043        0.151527\n",
       "239   thing         985   1287   6.0   0.005198       0.054241        0.124794\n",
       "210    work         887   1245   7.0   0.005029       0.059270        0.112378\n",
       "76     time         880   1147   8.0   0.004633       0.063903        0.111491\n",
       "242     use         826   1088   9.0   0.004395       0.068297        0.104650\n",
       "7      have         887   1078  10.0   0.004354       0.072651        0.112378"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most common words in non-toxic comments.\n",
    "nontoxic_wc_train = count(lemmas_train[~y_train_binary.astype(bool)])\n",
    "nontoxic_wc_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make comparison table.\n",
    "comp = pd.merge(toxic_wc_train[['word', 'appears_in_pct']], \n",
    "                nontoxic_wc_train[['word', 'appears_in_pct']], \n",
    "                how='outer', on='word', suffixes = ('_toxic', '_nontoxic'))\n",
    "\n",
    "comp = comp.fillna(0)\n",
    "\n",
    "comp['diff'] = abs(comp['appears_in_pct_toxic'] - comp['appears_in_pct_nontoxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize words with the biggest frequency difference between comment\n",
    "# categories.\n",
    "top_15 = comp.sort_values(by='diff', ascending=False).head(15)\n",
    "top_15.sort_values(by='appears_in_pct_toxic',\n",
    "                   ascending=False).plot.bar(x='word',\n",
    "                                             y=['appears_in_pct_toxic',\n",
    "                                                'appears_in_pct_nontoxic']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuck',\n",
       " 'shit',\n",
       " 'shitty',\n",
       " 'people',\n",
       " 'feel',\n",
       " 'idiot',\n",
       " 'bullshit',\n",
       " 'stupid',\n",
       " 'hell',\n",
       " 'seriously',\n",
       " 'right',\n",
       " 'asshole',\n",
       " 'crap',\n",
       " 'web',\n",
       " 'comment',\n",
       " 'man',\n",
       " 'different',\n",
       " 'ugly',\n",
       " 'care',\n",
       " 'read',\n",
       " 'damn',\n",
       " 'time',\n",
       " 'honest',\n",
       " 'not',\n",
       " 'yes',\n",
       " 'character',\n",
       " 'way',\n",
       " 'abuse',\n",
       " 'terrible',\n",
       " 'year',\n",
       " 'hate',\n",
       " 'begin',\n",
       " 'new',\n",
       " 'human',\n",
       " 'want',\n",
       " 'say',\n",
       " 'mean',\n",
       " 'interest',\n",
       " 'change',\n",
       " 'page',\n",
       " 'search',\n",
       " 'idiotic',\n",
       " 'website',\n",
       " 'datum',\n",
       " 'product',\n",
       " 'backwards',\n",
       " 'come',\n",
       " 'know',\n",
       " 'real',\n",
       " 'entire',\n",
       " 'god',\n",
       " 'case',\n",
       " 'prime',\n",
       " 'use',\n",
       " 'matt',\n",
       " 'criminal',\n",
       " 'buy',\n",
       " 'job',\n",
       " 'tell',\n",
       " 'fix',\n",
       " 'think',\n",
       " 'state',\n",
       " 'font',\n",
       " 'ui',\n",
       " 'googles',\n",
       " 'big',\n",
       " 'tv',\n",
       " 'black',\n",
       " 'error',\n",
       " 'help',\n",
       " 'call',\n",
       " 'complain',\n",
       " 'stuff',\n",
       " 'cost',\n",
       " 'apple',\n",
       " 'count',\n",
       " 'sorry',\n",
       " 'sure',\n",
       " 'user',\n",
       " 'support',\n",
       " 'app',\n",
       " 'death',\n",
       " 'edit',\n",
       " 'etc',\n",
       " 'problem',\n",
       " 'country',\n",
       " 'little',\n",
       " 'probably',\n",
       " 'deal',\n",
       " 'find',\n",
       " 'happen',\n",
       " 'microsoft',\n",
       " 'look',\n",
       " 'like',\n",
       " 'leave',\n",
       " 'language',\n",
       " 'idea',\n",
       " 'expect',\n",
       " 'understand',\n",
       " 'society',\n",
       " 'poor',\n",
       " 'provide',\n",
       " 'go',\n",
       " 'ask',\n",
       " 'asocial',\n",
       " 'blacklivesmatter',\n",
       " 'hermit',\n",
       " 'amazon',\n",
       " 'bitch',\n",
       " 'mauro',\n",
       " 'pretty',\n",
       " 'ditto',\n",
       " 'target',\n",
       " 'include',\n",
       " 'stop',\n",
       " 'child',\n",
       " 'public',\n",
       " 'exactly',\n",
       " 'term',\n",
       " 'spit',\n",
       " 'try',\n",
       " 'issue',\n",
       " 'information',\n",
       " 'source',\n",
       " 'youth',\n",
       " 'safari',\n",
       " 'constructive',\n",
       " 's',\n",
       " 'miss',\n",
       " 'skin',\n",
       " 'smell',\n",
       " 'cop',\n",
       " 'frustration',\n",
       " 'suppose',\n",
       " 'rest',\n",
       " 'attractive',\n",
       " 'similar',\n",
       " 'sense',\n",
       " 'discovery',\n",
       " 'business',\n",
       " 'assume',\n",
       " 'low',\n",
       " 'word',\n",
       " 'true',\n",
       " '3',\n",
       " 'place',\n",
       " 'guy',\n",
       " 'lean',\n",
       " 'eg',\n",
       " 'reason',\n",
       " 'animal',\n",
       " 'transition',\n",
       " 'index',\n",
       " 'linus',\n",
       " 'small',\n",
       " 'shut',\n",
       " 'vw',\n",
       " 'campaign',\n",
       " 'css',\n",
       " 'guess',\n",
       " 'actually',\n",
       " 'override',\n",
       " 'render',\n",
       " 'cancer',\n",
       " 'require',\n",
       " 'mouse',\n",
       " 'cheat',\n",
       " 'to',\n",
       " 'early',\n",
       " 'mac',\n",
       " 'send',\n",
       " 'exclude',\n",
       " 'test',\n",
       " 'world',\n",
       " 'screw',\n",
       " 'car',\n",
       " 'have',\n",
       " 'service',\n",
       " 'arm',\n",
       " 'firefox',\n",
       " 'claim',\n",
       " 'defend',\n",
       " 'wide',\n",
       " 'straight',\n",
       " 'designer',\n",
       " 'burn',\n",
       " 'file',\n",
       " 'limit',\n",
       " 'list',\n",
       " 'google',\n",
       " 'agree',\n",
       " 'argument',\n",
       " 'murder',\n",
       " 'flag',\n",
       " 'simple',\n",
       " 'actual',\n",
       " 'make',\n",
       " 'hand',\n",
       " 'eye',\n",
       " 'tiny',\n",
       " 'suck',\n",
       " 'twice',\n",
       " '1',\n",
       " 'rarely',\n",
       " 'javascript',\n",
       " 'wall',\n",
       " 'mistake',\n",
       " 'perspective',\n",
       " 'allow',\n",
       " 'encourage',\n",
       " 'feature',\n",
       " 'andor',\n",
       " 'fine',\n",
       " 'forget',\n",
       " 'logic',\n",
       " 'internet',\n",
       " 'difference',\n",
       " 'especially',\n",
       " 'damage',\n",
       " 'security',\n",
       " 'organization',\n",
       " 'street',\n",
       " 'free',\n",
       " 'supply',\n",
       " 'group',\n",
       " 'display',\n",
       " 'young',\n",
       " '10',\n",
       " 'late']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate feature wordlist from comparison table.\n",
    "autogen_wordlist = comp.sort_values(by='diff', ascending=False).head(229)['word']\n",
    "list(autogen_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature wordlist without stop words.\n",
    "autogen_wordlist = ['fuck',\n",
    "                    'shit',\n",
    "                    'shitty',\n",
    "                    'people',\n",
    "                    'feel',\n",
    "                    'idiot',\n",
    "                    'bullshit',\n",
    "                    'stupid',\n",
    "                    'hell',\n",
    "                    'seriously',\n",
    "                    'right',\n",
    "                    'asshole',\n",
    "                    'crap',\n",
    "                    'web',\n",
    "                    'comment',\n",
    "                    'man',\n",
    "                    'different',\n",
    "                    'ugly',\n",
    "                    'care',\n",
    "                    'read',\n",
    "                    'damn',\n",
    "                    'time',\n",
    "                    'honest',\n",
    "                    'not',\n",
    "                    'yes',\n",
    " 'character',\n",
    " 'way',\n",
    " 'abuse',\n",
    " 'terrible',\n",
    " 'year',\n",
    " 'hate',\n",
    " 'begin',\n",
    " 'new',\n",
    " 'human',\n",
    " 'want',\n",
    " 'say',\n",
    " 'mean',\n",
    " 'interest',\n",
    " 'change',\n",
    " 'page',\n",
    " 'search',\n",
    " 'idiotic',\n",
    " 'website',\n",
    " 'datum',\n",
    " 'product',\n",
    " 'backwards',\n",
    " 'come',\n",
    " 'know',\n",
    " 'real',\n",
    " 'entire',\n",
    " 'god',\n",
    " 'case',\n",
    " 'prime',\n",
    " 'use',\n",
    " 'matt',\n",
    " 'criminal',\n",
    " 'buy',\n",
    " 'job',\n",
    " 'tell',\n",
    " 'fix',\n",
    " 'think',\n",
    " 'state',\n",
    " 'font',\n",
    " 'ui',\n",
    " 'googles',\n",
    " 'big',\n",
    " 'tv',\n",
    " 'black',\n",
    " 'error',\n",
    " 'help',\n",
    " 'call',\n",
    " 'complain',\n",
    " 'stuff',\n",
    " 'cost',\n",
    " 'apple',\n",
    " 'count',\n",
    " 'sorry',\n",
    " 'sure',\n",
    " 'user',\n",
    " 'support',\n",
    " 'app',\n",
    " 'death',\n",
    " 'edit',\n",
    " 'etc',\n",
    " 'problem',\n",
    " 'country',\n",
    " 'little',\n",
    " 'probably',\n",
    " 'deal',\n",
    " 'find',\n",
    " 'happen',\n",
    " 'microsoft',\n",
    " 'look',\n",
    " 'like',\n",
    " 'leave',\n",
    " 'language',\n",
    " 'idea',\n",
    " 'expect',\n",
    " 'understand',\n",
    " 'society',\n",
    " 'poor',\n",
    " 'provide',\n",
    " 'go',\n",
    " 'ask',\n",
    " 'amazon',\n",
    "  'pretty',\n",
    " 'target',\n",
    " 'include',\n",
    " 'stop',\n",
    " 'child',\n",
    " 'public',\n",
    " 'exactly',\n",
    " 'term',\n",
    " 'spit',\n",
    " 'try',\n",
    " 'issue',\n",
    " 'information',\n",
    " 'source',\n",
    " 'youth',\n",
    " 'safari',\n",
    " 'constructive',\n",
    " 'miss',\n",
    " 'skin',\n",
    " 'smell',\n",
    " 'cop',\n",
    " 'frustration',\n",
    " 'suppose',\n",
    " 'rest',\n",
    " 'attractive',\n",
    " 'similar',\n",
    " 'sense',\n",
    " 'discovery',\n",
    " 'business',\n",
    " 'assume',\n",
    " 'low',\n",
    " 'word',\n",
    " 'true',\n",
    " 'place',\n",
    " 'guy',\n",
    " 'lean',\n",
    " 'eg',\n",
    " 'reason',\n",
    " 'animal',\n",
    " 'transition',\n",
    " 'index',\n",
    " 'linus',\n",
    " 'small',\n",
    " 'shut',\n",
    " 'vw',\n",
    " 'campaign',\n",
    " 'css',\n",
    " 'guess',\n",
    " 'actually',\n",
    " 'override',\n",
    " 'render',\n",
    " 'cancer',\n",
    " 'require',\n",
    " 'mouse',\n",
    " 'cheat',\n",
    " 'to',\n",
    " 'early',\n",
    " 'mac',\n",
    " 'send',\n",
    " 'exclude',\n",
    " 'test',\n",
    " 'world',\n",
    " 'screw',\n",
    " 'car',\n",
    " 'have',\n",
    " 'service',\n",
    " 'arm',\n",
    " 'firefox',\n",
    " 'claim',\n",
    " 'defend',\n",
    " 'wide',\n",
    " 'straight',\n",
    " 'designer',\n",
    " 'burn',\n",
    " 'file',\n",
    " 'limit',\n",
    " 'list',\n",
    " 'google',\n",
    " 'agree',\n",
    " 'argument',\n",
    " 'murder',\n",
    " 'flag',\n",
    " 'simple',\n",
    " 'actual',\n",
    " 'make',\n",
    " 'hand',\n",
    " 'eye',\n",
    " 'tiny',\n",
    " 'suck',\n",
    " 'twice',\n",
    " 'rarely',\n",
    " 'javascript',\n",
    " 'wall',\n",
    " 'mistake',\n",
    " 'perspective',\n",
    " 'allow',\n",
    " 'encourage',\n",
    " 'feature',\n",
    " 'andor',\n",
    " 'fine',\n",
    " 'forget',\n",
    " 'logic',\n",
    " 'internet',\n",
    " 'difference',\n",
    " 'especially',\n",
    " 'damage',\n",
    " 'security',\n",
    " 'organization',\n",
    " 'street',\n",
    " 'free',\n",
    " 'supply',\n",
    " 'group',\n",
    " 'display',\n",
    " 'young',\n",
    " '10',\n",
    " 'late']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autogen_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_train = dtm_train_orig[autogen_wordlist]\n",
    "dtm_test = dtm_test_orig[autogen_wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(512, input_dim=220, activation='relu'))\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x255fe665ac8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n",
    "model.fit(dtm_train, y_train_binary, epochs=25, \n",
    "          class_weight = {0: 0.010406, 1: 0.989594},\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2, \n",
    "          verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>7.976000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>8.345972e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.659985e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.490116e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2.161801e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.451471e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>9.868127e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9.999866e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  7.976000e+03\n",
       "mean   8.345972e-02\n",
       "std    1.659985e-01\n",
       "min    1.490116e-07\n",
       "25%    2.161801e-03\n",
       "50%    1.451471e-02\n",
       "75%    9.868127e-02\n",
       "max    9.999866e-01"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions.\n",
    "nn_train_pred = model.predict(dtm_train)\n",
    "nn_test_pred = model.predict(dtm_test)\n",
    "pd.DataFrame(nn_train_pred).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics on training dataset.\n",
    "train_accuracy = accuracy_score(y_train_binary, np.rint(nn_train_pred))\n",
    "train_recall = recall_score(y_train_binary, np.rint(nn_train_pred))\n",
    "train_precision = precision_score(y_train_binary, np.rint(nn_train_pred))\n",
    "\n",
    "# Calculate metrics on test dataset.\n",
    "test_accuracy = accuracy_score(y_test_binary, np.rint(nn_test_pred))\n",
    "test_recall = recall_score(y_test_binary, np.rint(nn_test_pred))\n",
    "test_precision = precision_score(y_test_binary, np.rint(nn_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9712\n",
      "Train recall: 0.8193\n",
      "Train precision: 0.2403 \n",
      "\n",
      "Test accuracy: 0.9584\n",
      "Test recall: 0.1500\n",
      "Test precision: 0.0435\n"
     ]
    }
   ],
   "source": [
    "# Display metrics for training dataset.\n",
    "print(f'Train accuracy: {train_accuracy:.4f}')\n",
    "print(f'Train recall: {train_recall:.4f}')\n",
    "print(f'Train precision: {train_precision:.4f}', '\\n')\n",
    "\n",
    "# Display metrics for test dataset.\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "print(f'Test recall: {test_recall:.4f}')\n",
    "print(f'Test precision: {test_precision:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft is one of the few companies in the world that will have this many people hitting them all at once. \n",
      "\n",
      "Have you considered the possibility that your cow-orkers hate it, and by extension, you? \n",
      "\n",
      "It's easy enough to try, why don't you give it a go and see for yourself. \n",
      "\n",
      "And if you do the disaster rehearsal thing too many times without anything happening, i.e. cry wolf, the next time no one pays any attention, Katrina or Sandy hits, and a lot of people die that didn't need to. \n",
      "\n",
      "Micro. Microsoft, Rebirth. Microsoft, Part 2. Microsoft, the Comeback. \n",
      "\n",
      "This just seems like a failing of the text editor. But, if you can't input the character...you can't input the character. \n",
      "\n",
      "The author mentions working on Safari. Perhaps writing code to deal with other people's crap has something to do with her views. \n",
      "\n",
      "Have you considered getting into the business of dealing with health insurance companies? \n",
      "\n",
      "> There are lab rats (whose hormones/genetics have been fucked with) that can literally die of starvation while remaining nearly spherical with excess fat. I bet there aren't. You'll need to cite that claim. \n",
      "\n",
      "He has lots of links. It's the web as TBL intended. \n",
      "\n",
      "You can't give up something you never had. \n",
      "\n",
      "And that's before you even consider building Microsoft Exchange ActiveSync... \n",
      "\n",
      "Corporations dissolve all the time. Do you have something else in mind when you say they cannot die? \n",
      "\n",
      "What? Having something for the sake of having something isn't productive. \n",
      "\n",
      "I have seen you post this several times so I am curious now. Why is this better than the others out there? \n",
      "\n",
      "If they're in the Five Eyes then no, they aren't any better. \n",
      "\n",
      "> I expect Reddit to be able to deliver... That was sarcasm, right? \n",
      "\n",
      "still not the more honest \"do no evil to our profits\" \n",
      "\n",
      "> You well and truly are a piece of shit. You can't comment like this here. We've banned this account. \n",
      "\n",
      "It doesn't seem to, but https://historio.us/ does. \n",
      "\n",
      "It's always a lot easier to talk about human loss when it happens to somebody else and in the abstract. \n",
      "\n",
      "just tried it out - it caused iterm to crash \n",
      "\n",
      "Really means a lot. If you ever need a confidence boost though you should checkout 3000[0], I hear that developer is fucking killing it. [0]localhost:3000 \n",
      "\n",
      "And I'll take the black cats, which are also less likely to be adopted. To make up for that, August 17 is Black Cat Appreciation Day! [1] [2] #BlackCatsMatter #BlackDogsMatter [1] https://www.aspca.org/blog/celebrate-black-cat-appreciation-... [2] https://www.daysoftheyear.com/days/black-cat-appreciation-da... \n",
      "\n",
      "It's getting more and more possible. Instagram, 10-13 employees at time of acquisition. Were 5 for most of the growth. WhatsApp 30 people 19bn. \n",
      "\n",
      "Most likely, they'll get the same results as if they were scanning from an EC2 machine. I doubt Amazon would put it in a trusted (V)LAN. \n",
      "\n",
      "Integrating with the existing web is definitely one of the goals, see https://github.com/ipfs/ipfs.js \n",
      "\n",
      "That's what I had people do. \n",
      "\n",
      "Cool technology, but what do you do with the character once it is digital? It would inspire more creativity if maybe once kids uploaded their characters they could create a story or something along those lines. \n",
      "\n",
      "I think we should be aiming for major positive scenario. We don't have much choice, otherwise genetic malfunctions will get us sooner or later. The way we're having children now (few, low child mortality) is a way to breed genetic defects. \n",
      "\n",
      "Sorry. \"measure or control\" \n",
      "\n",
      "Ouch. So much for a tech meritocracy, right? \n",
      "\n",
      "Well Apple just announced a device chasing the Microsoft Surface, so I'd say MS is back on track. \n",
      "\n",
      "Finally, a way out of callback hell. Now, make this work with IndexedDB and I might revisit IDB. \n",
      "\n",
      "AdBlock Plus does the same stuff, you might consider switching to uBlock for Safari. \n",
      "\n",
      "A link to this gist really should go in the OP by 'whoishiring. It's damn useful. \n",
      "\n",
      "Well, Linus succeeds in keeping off crap code because he is not afraid calling crap crap. Might be it appears hostile, but it forces people to submit quality code. \n",
      "\n",
      "People have raised this issue before, but it hasn't been taken seriously: If Google can't show its own products ahead of others in the Google search, why can Amazon show its own products ahead of all the other products on its store on its front-page? \n",
      "\n",
      "Thanks again! Let's have another date. Email me! edit: Update: It's happening!!!!!! Thanks, HN. \n",
      "\n",
      "In a world of land value tax. \n",
      "\n",
      "Seems Microsoft made the patent \"deal\" with Google just in time. Soon they wouldn't have had any leverage to \"threaten\" companies with patent lawsuits. \n",
      "\n",
      "That's the entire premise. It's econ101. \n",
      "\n",
      "lol seriously. What crosses your head when you just remove the scrollbar? \n",
      "\n",
      "I'd really hate to be in the same space as amazon. \n",
      "\n",
      "Carer. My neighbor is one for her two disabled children. She sells art and things on ebay to make ends meet. \n",
      "\n",
      "I seem to recall that at least around 1980, the Guinness Book of World Records said that the hardest tongue-twister in the world was a Czech phrase that translates \"stick a finger in the throat\". The whole phrase has no vowels in Czech. \n",
      "\n",
      "Amazon _Prime_ almost never has new releases. Can you point to any recent release that's on Prime right now? \n",
      "\n",
      "Unless they are programming under the guise of a couple gods of the industry. See: Jamie Zawinski The people he worked with gave him far more than most graduate degrees ever would have. \n",
      "\n",
      "Are we saying that a \"southern high school\" is an environment to which no civilized human being should be subjected? b^) \n",
      "\n",
      "I just see a blank grey screen, because the page doesn't display if you have Facebook Connect blocked. \n",
      "\n",
      "Some employers have deals with banks to vouch for you when you don't have a score. I got a US credit card when I moved here and bootstrapped my credit profile from that. \n",
      "\n",
      "The e-ink display showing a shipping label is brilliant. \n",
      "\n",
      "As much as a fan of Gates I am, not a sustainable/efficient system when power is fairly centralised and can be bought. \n",
      "\n",
      "No, it's that these listings on Amazon are kind-of-like \"eBay\" just without the auctioning part. The seller can write whatever he wants. Amazon just evaluates the complaints of the buyers. \n",
      "\n",
      " # wc -m Last I checked; thought leaders don't convey their thoughts in 140 characters or less. Nothing of really value is that short. Sorry. 133 \n",
      "\n",
      "> What is it like to have never felt an emotion? Since for it to be something you'd have to feel an emotion regarding your condition, it's nothing (neutral). \n",
      "\n",
      "Burning produces CO2 and toxic fumes. Worms produce CO2 and fertilizer. \n",
      "\n",
      "Among her later projects was Apple's Safari. Writing code that must absolutely positively do the right thing ... with other people's unvalidated crap ... might just colour your views slightly. \n",
      "\n",
      "@Curiousjorge.. Never, Never quit - Winston Churchill. And he also said when you are going through hell, you keep going. \n",
      "\n",
      "Telling people how they should feel is not very nice either. \n",
      "\n",
      "If England hadn't kicked off the industrial revolution it might have taken another few centuries to start and you'd be shitting in a hole in the ground in the dark right now. So be grateful for the huge gift they gave all of us. \n",
      "\n",
      "Like what happened to AdBlockPlus, now being owned by UnitedInternet, the company that invented BinLayer (in-page pop-ups)? \n",
      "\n",
      "My first thought was Basho's logo, since they both have something sticking out of the top left of the character's head. \n",
      "\n",
      "Peanut butter in porridge you say, I shall have to try this! Add some honey and a banana too and you could call it \"Elvis porridge\". \n",
      "\n",
      "Just tiny :P \n",
      "\n",
      "> These threads always fucking suck on HN. Please don't post things like that to HN. They only make threads suck worse. \n",
      "\n",
      "> If Apple had the decency to ship phones with a 24h+ battery Huh? I hate my iPhone 6 but the battery has never been an issue. I've gone 24h+ multiple times. \n",
      "\n",
      "If you want to render an element with transparency, you have to seperate it from the page and render it yourself with ctx.drawWindow(window, x, y, dx, dy, \"transparent\") to a canvas \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be honest, they don't seem to be very proficient at using Rust properly. Of course, this means that the documentation or other instruction materials might need to be improved. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display comments identified as toxic by custom neural network.\n",
    "for text in X_test[np.rint(nn_test_pred).squeeze().astype(bool)]:\n",
    "    print(text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once you take money from investors, you have both a legal and a moral responsibility to them. It's important to remember that selling equity is really selling something, just like selling a product. You can't sell something, take somebody's money, and then just not deliver. What you are selling to investors, generally, is return on investment. The real problem comes in because founders and VCs have different understandings of \"grow fast\". There will come choices that have different levels of risk. VCs always want you to take the high-risk, high-reward path, because they have a portfolio company and are looking for the few really big winners. You, on the other hand, are trying to build something (team, community, product, etc), and so you will favor slower but safer growth. I guess in theory you could sell VCs on a high-growth approach and then turn around and say, \"Ha ha, fuck you, we're going to keep your money and try to grow modestly.\" But a) VCs are way better at their game than you are, so they're hard to fool, and b) then you're an asshole, which is bad on its own and also has consequences for future relationships (and future funding). Plus, if you're in a VC-fundable space, you'll probably have competitors who are also taking money, and they'll be able to outspend you on future rounds. \n",
      "\n",
      "> There are lab rats (whose hormones/genetics have been fucked with) that can literally die of starvation while remaining nearly spherical with excess fat. I bet there aren't. You'll need to cite that claim. \n",
      "\n",
      "Yeah, and also: Who gives a shit? It's the convention they chose, just adapt to it and get over it, or program in a different language. People who like to complain about PHP are rarely the same people who use it every day. \n",
      "\n",
      "Really means a lot. If you ever need a confidence boost though you should checkout 3000[0], I hear that developer is fucking killing it. [0]localhost:3000 \n",
      "\n",
      "As as society, we're all agitated about dinner because we're trained to be by billions in ad spending. Part of that is a self loathing of one of the key elements of the dinner meal -- culture. Nothing is good enough. My sister is a good example of this. She picked up a crazy, pain in the ass fad diet, has two kids and high powered corporate gig. Her husband is in a similar gig. The kids are in care from 7-7. So yeah, even buying a vegan, glueten free burrito is a pain in the ass. I worry about her, as she's going to have an anxiety attack someday. \n",
      "\n",
      "It's overstepping Facebook's bounds. While legal, it's rather rude and just shows what a shitty service they are: don't trust them with your info. At best, it's a mismatch between what Facebook thinks it is and what users are led to believe. Though, in practise, it's doubtful that it's actually company policy to do this. It's most likely the result of crappy support departments, overloaded with tons of requests. (Users are mostly idiots, so most tickets are of poor quality, and it leads to being aggressive in \"resolving\" tickets, even when the user is fine and in the right.) \n",
      "\n",
      "Easy fix 1 - Vote on the god damn weekend. I can't vote because my boss won't give me time off is the sort of shit that should have been on the scrap heap alongside slavery. Less easy fix 2 - make voting compulsory, with a small fine, like $20/$40 for not showing up to vote. Remember I said showing up, not actually voting, since you can simply cast a blank ballot and then leave having done nothing just as if you didn't vote. \n",
      "\n",
      "Well, a real-life example would contain both disrespect and information about the mistakes, and how to do better. Something more like, 'How could you possibly be so stupid as to use the same memory location to store the car's current radio station and the desired fuel-injection rate, controlled only by a flag in another piece of memory? Didn't it occur to you that someone working on the entertainment system code might never even think of checking for a flag related to speed & safety? Dear God, Smith, do you realise how many people you could have killed? I can only hope that your other mistakes have prevented idiots like you from being born and placed into positions of trust, you sorry misbegotten excuse for an engineer!' Harsh, but I bet Smith would remember that moment well, and be more careful in the future. Or he might quit, and never write automotive-safety-impacting code again. \n",
      "\n",
      "I'm not sure what people find \"beautiful\" or \"touching\" in this story. A man is wracked by guilt over what he has done to the point of deciding to end his life, but then he forgets all about it as soon as a pretty girl gives him attention. Inasmuch as it is believable, this is a story about how all our lofty notions of justice, honor, purpose etc. are just bullshit, and it's really all about fucking, consuming and keeping those genes alive. Matter over mind, humanity revealed as a mere dusting on thought over the throbbing mass of limbic functions. It is depressing. Its only redeeming quality is the fact that it's obviously fake. \n",
      "\n",
      "There is already a tinychat (and the existing tinychat are scumbags) so a name change might be useful. \n",
      "\n",
      "> When everything is communicated in exactly the same \"business professional\" tone, you have effectively reduced the bit depth. That Linus calls any attempt at civility \"bourgeois mores\" or something like that doesn't make it so. No one is calling for the same \"business professional\" tone. Just for basic civility. You can be a little rude, just don't go overboard. > The way you deal with things is by being logical and having supporting evidence, but I don't believe communities have to be for everyone and work for everyone. Again, this is the response I'd expect if someone called for mandatory uniforms. Here someone is just asking them to be itsy-bitsy-slightly less dickish. To tone down their dickishness. To go from being total dicks to being normal dicks. Ah, but who decides what is a normal, acceptable amount of dickishness? This is actually quite simple: common sense, combined with listening to what members of the community say (sprinkled with a well deserved apology here and there). And when common sense doesn't cut it any more, there is a need for rules. Namely, the exact same way every reasonably functioning society that wished to be anything less than a total asshole-heaven has done since the dawn of civilization has done it. \n",
      "\n",
      "Um, no offense, why? They crushed all the technical bookstores because they didn't have to pay sales tax. Their customer service beyond cheap refunds is abysmal. They understand how hard it is to distinguish between stuff that is only serviced through them, but choose not to fix it. And Bezos is famous for being both an asshole and micromanager. One-stop? Sure. Efficient? Maybe. Trustworthy? Pardon me for laughing. \n",
      "\n",
      "> It's a piece of shit because every little thing you use has 3 billion dependencies which you have to validate and verify n some manner. If you mean to prevent breakage, shrinkwrap lets you lock down dependency versions: https://docs.npmjs.com/cli/shrinkwrap If you mean something else (security?) then I'd probably just not use npm, because as you said your dependency tree is going to be deep and reviewing all of that code is impossible. \n",
      "\n",
      "That's just plain stupid. I get it, you don't want EVs to be \"tax-free\", but at least make it smaller than on gas-powered cars to encourage people to buy cleaner cars. When EVs are 50% of the market, then you can make it equal, if you think you're losing money or whatever. But such high taxes already seem extreme for any car anyway. They also eliminated the tax on NOx, even after the VW scandal. Ugh. I was wrong. This isn't stupid. Just malicious/corrupt. \n",
      "\n",
      "Is this a religious discussion now? I'm not sure that applies, anyway; ISTM that Jobs is already a dung beetle, or whatever. Besides, TFA is hardly \"seeking revenge\". The tone isn't even particularly harsh. It's just: here's some shit that dude did. \n",
      "\n",
      "I agree with the OP. Vice is pretty much a trash site. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to throw Barney \"Roll the Dice\" Frank in jail? There tend to be lots of unintended consequences to punishing stupid politicians like that, or people outside the government following the government's prior orders. \n",
      "\n",
      "Alternately, once they realized it wasn't working, announce that the bounty would end in a month, so that the breeders would hustle to kill all their cobras and bring them in. (Or, to shift a little blame away from the British, maybe if the breeders hadn't been stupid/dickish enough to release a fuckload of live cobras in their neighbors' back yards.) \n",
      "\n",
      "> These threads always fucking suck on HN. Please don't post things like that to HN. They only make threads suck worse. \n",
      "\n",
      "Edit: looking at the reading list on the site I might stop mocking the \"programming, motherfucker\" crowd. There's a lot of good works on design, analysis, and engineering techniques referenced. So, the cool, catchy line is misleading and even they will depend on Things Other Than Programming. Hope the site gets more popular. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display comments predicted possibly toxic by PerspectiveAPI (target).\n",
    "for text in X_test[y_test_binary.astype(bool)]:\n",
    "    print(text, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
