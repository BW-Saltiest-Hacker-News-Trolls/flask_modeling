{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "df = pd.read_csv('first_week_oct_2015_comments_by_top_400_with_scores_and_features_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent</th>\n",
       "      <th>by</th>\n",
       "      <th>time</th>\n",
       "      <th>hour_posted</th>\n",
       "      <th>text</th>\n",
       "      <th>dead</th>\n",
       "      <th>ranking</th>\n",
       "      <th>text_len</th>\n",
       "      <th>pct_caps</th>\n",
       "      <th>tags_per_char</th>\n",
       "      <th>papi_toxicity</th>\n",
       "      <th>v_neg</th>\n",
       "      <th>v_neu</th>\n",
       "      <th>v_pos</th>\n",
       "      <th>v_compound</th>\n",
       "      <th>tb_polarity</th>\n",
       "      <th>tb_subjectivity</th>\n",
       "      <th>tb_nb_prob_neg</th>\n",
       "      <th>pc_prob_offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10331981</td>\n",
       "      <td>10331895</td>\n",
       "      <td>debacle</td>\n",
       "      <td>2015-10-05 14:24:42+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>US is not really scared by BRICS at all. They'...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.61875</td>\n",
       "      <td>0.080001</td>\n",
       "      <td>0.238871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10343811</td>\n",
       "      <td>10343761</td>\n",
       "      <td>sarciszewski</td>\n",
       "      <td>2015-10-07 02:13:15+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>I wasn't really trying to argue, they said the...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048637</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3947</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.593498</td>\n",
       "      <td>0.050161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10331538</td>\n",
       "      <td>10331008</td>\n",
       "      <td>debacle</td>\n",
       "      <td>2015-10-05 13:08:10+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>The examples on the homepage kind of underscor...</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>88</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044777</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.336201</td>\n",
       "      <td>0.098511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10340097</td>\n",
       "      <td>10339965</td>\n",
       "      <td>debacle</td>\n",
       "      <td>2015-10-06 16:33:06+00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>No mention of a critical aspect of a service l...</td>\n",
       "      <td>False</td>\n",
       "      <td>22</td>\n",
       "      <td>99</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035335</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>0.352414</td>\n",
       "      <td>0.056323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10338552</td>\n",
       "      <td>10337763</td>\n",
       "      <td>debacle</td>\n",
       "      <td>2015-10-06 13:06:26+00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>I think some of these points are gross exagger...</td>\n",
       "      <td>False</td>\n",
       "      <td>38</td>\n",
       "      <td>868</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>0.232577</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.8233</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.001499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    parent            by                       time  hour_posted  \\\n",
       "0  10331981  10331895       debacle  2015-10-05 14:24:42+00:00           14   \n",
       "1  10343811  10343761  sarciszewski  2015-10-07 02:13:15+00:00            2   \n",
       "2  10331538  10331008       debacle  2015-10-05 13:08:10+00:00           13   \n",
       "3  10340097  10339965       debacle  2015-10-06 16:33:06+00:00           16   \n",
       "4  10338552  10337763       debacle  2015-10-06 13:06:26+00:00           13   \n",
       "\n",
       "                                                text   dead  ranking  \\\n",
       "0  US is not really scared by BRICS at all. They'...  False        0   \n",
       "1  I wasn't really trying to argue, they said the...  False        0   \n",
       "2  The examples on the homepage kind of underscor...  False       12   \n",
       "3  No mention of a critical aspect of a service l...  False       22   \n",
       "4  I think some of these points are gross exagger...  False       38   \n",
       "\n",
       "   text_len  pct_caps  tags_per_char  papi_toxicity  v_neg  v_neu  v_pos  \\\n",
       "0       146  0.068493       0.000000       0.100881  0.000  0.744  0.256   \n",
       "1        76  0.013158       0.000000       0.048637  0.195  0.805  0.000   \n",
       "2        88  0.034091       0.000000       0.044777  0.000  0.864  0.136   \n",
       "3        99  0.010101       0.000000       0.035335  0.214  0.667  0.119   \n",
       "4       868  0.013825       0.009217       0.232577  0.081  0.902  0.017   \n",
       "\n",
       "   v_compound  tb_polarity  tb_subjectivity  tb_nb_prob_neg  pc_prob_offensive  \n",
       "0      0.7859     0.156250          0.61875        0.080001           0.238871  \n",
       "1     -0.3947     0.200000          0.20000        0.593498           0.050161  \n",
       "2      0.2975     0.137500          0.50000        0.336201           0.098511  \n",
       "3     -0.2500     0.000000          0.80000        0.352414           0.056323  \n",
       "4     -0.8233     0.076667          0.26000        0.000790           0.001499  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'parent', 'by', 'time', 'hour_posted', 'text', 'dead', 'ranking',\n",
       "       'text_len', 'pct_caps', 'tags_per_char', 'papi_toxicity', 'v_neg',\n",
       "       'v_neu', 'v_pos', 'v_compound', 'tb_polarity', 'tb_subjectivity',\n",
       "       'tb_nb_prob_neg', 'pc_prob_offensive'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review column list.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target column.\n",
    "y = df['papi_toxicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test subsets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], \n",
    "                                                    y, \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert comment text to lowercase, remove punctuation, and trim \n",
    "# whitespace.\n",
    "simple_text_train = X_train.str.lower()\n",
    "simple_text_train = simple_text_train.str.replace('[{}]'.format(string.punctuation), '')\n",
    "simple_text_train = simple_text_train.str.replace('\\s+', ' ', regex=True)\n",
    "simple_text_train = simple_text_train.str.strip()\n",
    "\n",
    "simple_text_test = X_test.str.lower()\n",
    "simple_text_test = simple_text_test.str.replace('[{}]'.format(string.punctuation), '')\n",
    "simple_text_test = simple_text_test.str.replace('\\s+', ' ', regex=True)\n",
    "simple_text_test = simple_text_test.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without the safe harbor agreement you can no longer avoid eu privacy regulations by storing the data in the us maybe im missing something here my understanding is that the safe harbour agreement wasnt a mechanism for us companies to avoid eu data protection regulations it was a certification that they did comply with eu data protection particularly in situations where that data was transmitted outside the eu now its gone eu customer data held by us companies will be governed by national data protection laws instead so may end up having to be stored within the eu the us privacy regulations are no longer considered compatible with the eu privacy regulations i dont think they ever were which is why the safe harbour needed to exist in the first place \n",
      "\n",
      "imagine how much a interlispd smalltalk mesacedar workstation would have cost in the 70s versus a plain pdp11 eh if theyd put altos in serial production instead of small batches at a time adding up to 2000 units it wouldnt have been vastly more expensive there was nothing both exotic and wildly expensive about them compared to contemporary pdp11s a graphics console the extras were a mouse chord keyboard network adapter and more memory per person than normal for a pdp11 \n",
      "\n",
      "thanks for the links as the current defacto maintainer of ppi the answer is rather simple pt predates ppi by almost two years httpsmetacpanorgsourceshancockperltidy20021130chan httpsmetacpanorgsourceadamkppi01changes and for the longest time perl was considered unparsable primarily due to the two features of function parens being optional and the argumentslurpiness of function calls being unknowable without introspecting the function reference that ends up being the final one at runtime in the most famous example this can lead to a after a function call being considered either the division operator or the start of a regex with both interpretations resulting in valid perl code it took a while for anyone to come up with a schema in which perl could be parsed while also being roundtrippable it took ppi a while to get there and be stable and meanwhile pt had already become stable itself \n",
      "\n",
      "surely you can give a rough idea \n",
      "\n",
      "disclaimer im a browser engine developer not a frontend or fullstack web developer that is why you have a sane view on the wed dev trenches more is better we are all insane ppl who think adding react to a page will make it spiffier \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check results.\n",
    "for text in simple_text_train.head():\n",
    "    print(text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text.\n",
    "tokens_train = simple_text_train.apply(nlp.tokenizer)\n",
    "tokens_test = simple_text_test.apply(nlp.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize; remove stop words.\n",
    "lemmas_train = tokens_train.apply(lambda x: [token.lemma_ for token in x if not token.is_stop])\n",
    "lemmas_test = tokens_test.apply(lambda x: [token.lemma_ for token in x if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lemmatize; leave stop words. Picks up \"you\" and \"your\" as potentially \"toxic\" words.\n",
    "# lemmas_train = tokens_train.apply(lambda x: [token.lemma_ for token in x])\n",
    "# lemmas_test = tokens_test.apply(lambda x: [token.lemma_ for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1362    [safe, harbor, agreement, long, avoid, eu, pri...\n",
       "7304    [imagine, interlispd, smalltalk, mesacedar, wo...\n",
       "4237    [thank, link, current, defacto, maintainer, pp...\n",
       "3680                                [surely, rough, idea]\n",
       "4780    [disclaimer, be, browser, engine, developer, f...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results.\n",
    "lemmas_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>101</th>\n",
       "      <th>10k</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3588 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100  1000  101  10k   11   12  120   13   14  ...  yes  yesterday  \\\n",
       "0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "1  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "2  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "3  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "4  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0        0.0   \n",
       "\n",
       "   yield  york  young  youth  youtube  zero  zillow  zone  \n",
       "0    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "1    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "2    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "3    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "4    0.0   0.0    0.0    0.0      0.0   0.0     0.0   0.0  \n",
       "\n",
       "[5 rows x 3588 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate vectorizer object.\n",
    "tfidf = TfidfVectorizer(min_df=0.001, max_df=0.999)\n",
    "\n",
    "# Create a vocabulary and get word counts per document.\n",
    "dtm_train = tfidf.fit_transform(lemmas_train.astype(str))\n",
    "dtm_test = tfidf.transform(lemmas_test.astype(str))\n",
    "\n",
    "# Get feature names to use as dataframe column headers.\n",
    "dtm_train_orig = pd.DataFrame(dtm_train.todense(), columns=tfidf.get_feature_names())\n",
    "dtm_test_orig = pd.DataFrame(dtm_test.todense(), columns=tfidf.get_feature_names())\n",
    "\n",
    "# Copy document term matrices.\n",
    "dtm_train = dtm_train_orig.copy()\n",
    "dtm_test = dtm_test_orig.copy()\n",
    "\n",
    "# Preview feature matrix.\n",
    "dtm_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated feature (word) selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PerspectiveAPI toxicity scores to binary class labels.\n",
    "y_train_binary = (y_train > 0.7).astype(int)\n",
    "y_test_binary = (y_test > 0.7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7893,   83], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check target distribution.\n",
    "np.bincount(y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(docs):\n",
    "    \"\"\"\n",
    "    Helper function for feature selection.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    appears_in = Counter()\n",
    "    \n",
    "    total_docs = len(docs)\n",
    "\n",
    "    for doc in docs:\n",
    "        word_counts.update(doc)\n",
    "        appears_in.update(set(doc))\n",
    "\n",
    "    temp = zip(word_counts.keys(), word_counts.values())\n",
    "      \n",
    "    wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
    "\n",
    "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
    "    total = wc['count'].sum()\n",
    "\n",
    "    wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
    "        \n",
    "    wc = wc.sort_values(by='rank')\n",
    "    wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
    "\n",
    "    t2 = zip(appears_in.keys(), appears_in.values())\n",
    "    ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
    "    wc = ac.merge(wc, on='word')\n",
    "\n",
    "    wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
    "        \n",
    "    return wc.sort_values(by='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>appears_in</th>\n",
       "      <th>count</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_total</th>\n",
       "      <th>cul_pct_total</th>\n",
       "      <th>appears_in_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>not</td>\n",
       "      <td>35</td>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.421687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fuck</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.029548</td>\n",
       "      <td>0.253012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>people</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.039679</td>\n",
       "      <td>0.265060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>shit</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.049388</td>\n",
       "      <td>0.253012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>like</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.156627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>s</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>0.066695</td>\n",
       "      <td>0.180723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>amazon</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.007176</td>\n",
       "      <td>0.073871</td>\n",
       "      <td>0.036145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>time</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.080625</td>\n",
       "      <td>0.156627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>work</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.087379</td>\n",
       "      <td>0.108434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>have</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.093710</td>\n",
       "      <td>0.132530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  appears_in  count  rank  pct_total  cul_pct_total  appears_in_pct\n",
       "21      not          35     46   1.0   0.019417       0.019417        0.421687\n",
       "3      fuck          21     24   2.0   0.010131       0.029548        0.253012\n",
       "42   people          22     24   3.0   0.010131       0.039679        0.265060\n",
       "52     shit          21     23   4.0   0.009709       0.049388        0.253012\n",
       "181    like          13     21   5.0   0.008864       0.058252        0.156627\n",
       "50        s          15     20   6.0   0.008442       0.066695        0.180723\n",
       "929  amazon           3     17   7.0   0.007176       0.073871        0.036145\n",
       "44     time          13     16   8.0   0.006754       0.080625        0.156627\n",
       "200    work           9     16   9.0   0.006754       0.087379        0.108434\n",
       "102    have          11     15  10.0   0.006332       0.093710        0.132530"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most common words in toxic comments.\n",
    "toxic_wc_train = count(lemmas_train[y_train_binary.astype(bool)])\n",
    "toxic_wc_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>appears_in</th>\n",
       "      <th>count</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_total</th>\n",
       "      <th>cul_pct_total</th>\n",
       "      <th>appears_in_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>not</td>\n",
       "      <td>2984</td>\n",
       "      <td>4872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>0.378057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>people</td>\n",
       "      <td>1485</td>\n",
       "      <td>2272</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>0.188141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>like</td>\n",
       "      <td>1439</td>\n",
       "      <td>1900</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.182313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>s</td>\n",
       "      <td>1246</td>\n",
       "      <td>1590</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.042952</td>\n",
       "      <td>0.157861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>think</td>\n",
       "      <td>1196</td>\n",
       "      <td>1508</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>0.049043</td>\n",
       "      <td>0.151527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>thing</td>\n",
       "      <td>985</td>\n",
       "      <td>1287</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.124794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>work</td>\n",
       "      <td>887</td>\n",
       "      <td>1245</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.059270</td>\n",
       "      <td>0.112378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>time</td>\n",
       "      <td>880</td>\n",
       "      <td>1147</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.111491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>use</td>\n",
       "      <td>826</td>\n",
       "      <td>1088</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>0.068297</td>\n",
       "      <td>0.104650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>have</td>\n",
       "      <td>887</td>\n",
       "      <td>1078</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.072651</td>\n",
       "      <td>0.112378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  appears_in  count  rank  pct_total  cul_pct_total  appears_in_pct\n",
       "25      not        2984   4872   1.0   0.019678       0.019678        0.378057\n",
       "230  people        1485   2272   2.0   0.009177       0.028855        0.188141\n",
       "375    like        1439   1900   3.0   0.007674       0.036530        0.182313\n",
       "198       s        1246   1590   4.0   0.006422       0.042952        0.157861\n",
       "0     think        1196   1508   5.0   0.006091       0.049043        0.151527\n",
       "233   thing         985   1287   6.0   0.005198       0.054241        0.124794\n",
       "184    work         887   1245   7.0   0.005029       0.059270        0.112378\n",
       "72     time         880   1147   8.0   0.004633       0.063903        0.111491\n",
       "255     use         826   1088   9.0   0.004395       0.068297        0.104650\n",
       "37     have         887   1078  10.0   0.004354       0.072651        0.112378"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find most common words in non-toxic comments.\n",
    "nontoxic_wc_train = count(lemmas_train[~y_train_binary.astype(bool)])\n",
    "nontoxic_wc_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make comparison table.\n",
    "comp = pd.merge(toxic_wc_train[['word', 'appears_in_pct']], \n",
    "                nontoxic_wc_train[['word', 'appears_in_pct']], \n",
    "                how='outer', on='word', suffixes = ('_toxic', '_nontoxic'))\n",
    "\n",
    "comp = comp.fillna(0)\n",
    "\n",
    "comp['diff'] = abs(comp['appears_in_pct_toxic'] - comp['appears_in_pct_nontoxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize words with the biggest frequency difference between comment\n",
    "# categories.\n",
    "top_15 = comp.sort_values(by='diff', ascending=False).head(15)\n",
    "top_15.sort_values(by='appears_in_pct_toxic',\n",
    "                   ascending=False).plot.bar(x='word',\n",
    "                                             y=['appears_in_pct_toxic',\n",
    "                                                'appears_in_pct_nontoxic']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuck',\n",
       " 'shit',\n",
       " 'shitty',\n",
       " 'people',\n",
       " 'feel',\n",
       " 'idiot',\n",
       " 'bullshit',\n",
       " 'stupid',\n",
       " 'hell',\n",
       " 'seriously',\n",
       " 'right',\n",
       " 'asshole',\n",
       " 'crap',\n",
       " 'web',\n",
       " 'comment',\n",
       " 'man',\n",
       " 'different',\n",
       " 'ugly',\n",
       " 'care',\n",
       " 'read',\n",
       " 'damn',\n",
       " 'time',\n",
       " 'honest',\n",
       " 'not',\n",
       " 'yes',\n",
       " 'character',\n",
       " 'way',\n",
       " 'abuse',\n",
       " 'terrible',\n",
       " 'year',\n",
       " 'hate',\n",
       " 'begin',\n",
       " 'new',\n",
       " 'human',\n",
       " 'want',\n",
       " 'say',\n",
       " 'mean',\n",
       " 'interest',\n",
       " 'change',\n",
       " 'page',\n",
       " 'search',\n",
       " 'idiotic',\n",
       " 'website',\n",
       " 'datum',\n",
       " 'product',\n",
       " 'backwards',\n",
       " 'come',\n",
       " 'know',\n",
       " 'real',\n",
       " 'entire',\n",
       " 'god',\n",
       " 'case',\n",
       " 'prime',\n",
       " 'use',\n",
       " 'matt',\n",
       " 'criminal',\n",
       " 'buy',\n",
       " 'job',\n",
       " 'tell',\n",
       " 'fix',\n",
       " 'think',\n",
       " 'state',\n",
       " 'font',\n",
       " 'ui',\n",
       " 'googles',\n",
       " 'big',\n",
       " 'tv',\n",
       " 'black',\n",
       " 'error',\n",
       " 'help',\n",
       " 'call',\n",
       " 'complain',\n",
       " 'stuff',\n",
       " 'cost',\n",
       " 'apple',\n",
       " 'count',\n",
       " 'sorry',\n",
       " 'sure',\n",
       " 'user',\n",
       " 'support',\n",
       " 'app',\n",
       " 'death',\n",
       " 'edit',\n",
       " 'etc',\n",
       " 'problem',\n",
       " 'country',\n",
       " 'little',\n",
       " 'probably',\n",
       " 'deal',\n",
       " 'find',\n",
       " 'happen',\n",
       " 'microsoft',\n",
       " 'look',\n",
       " 'like',\n",
       " 'leave',\n",
       " 'language',\n",
       " 'idea',\n",
       " 'expect',\n",
       " 'understand',\n",
       " 'society',\n",
       " 'poor',\n",
       " 'provide',\n",
       " 'go',\n",
       " 'ask',\n",
       " 'asocial',\n",
       " 'blacklivesmatter',\n",
       " 'hermit',\n",
       " 'amazon',\n",
       " 'bitch',\n",
       " 'mauro',\n",
       " 'pretty',\n",
       " 'ditto',\n",
       " 'target',\n",
       " 'include',\n",
       " 'stop',\n",
       " 'child',\n",
       " 'public',\n",
       " 'exactly',\n",
       " 'term',\n",
       " 'spit',\n",
       " 'try',\n",
       " 'issue',\n",
       " 'information',\n",
       " 'source',\n",
       " 'youth',\n",
       " 'safari',\n",
       " 'constructive',\n",
       " 's',\n",
       " 'miss',\n",
       " 'skin',\n",
       " 'smell',\n",
       " 'cop',\n",
       " 'frustration',\n",
       " 'suppose',\n",
       " 'rest',\n",
       " 'attractive',\n",
       " 'similar',\n",
       " 'sense',\n",
       " 'discovery',\n",
       " 'business',\n",
       " 'assume',\n",
       " 'low',\n",
       " 'word',\n",
       " 'true',\n",
       " '3',\n",
       " 'place',\n",
       " 'guy',\n",
       " 'lean',\n",
       " 'eg',\n",
       " 'reason',\n",
       " 'animal',\n",
       " 'transition',\n",
       " 'index',\n",
       " 'linus',\n",
       " 'small',\n",
       " 'shut',\n",
       " 'vw',\n",
       " 'campaign',\n",
       " 'css',\n",
       " 'guess',\n",
       " 'actually',\n",
       " 'override',\n",
       " 'render',\n",
       " 'cancer',\n",
       " 'require',\n",
       " 'mouse',\n",
       " 'cheat',\n",
       " 'to',\n",
       " 'early',\n",
       " 'mac',\n",
       " 'send',\n",
       " 'exclude',\n",
       " 'test',\n",
       " 'world',\n",
       " 'screw',\n",
       " 'car',\n",
       " 'have',\n",
       " 'service',\n",
       " 'arm',\n",
       " 'firefox',\n",
       " 'claim',\n",
       " 'defend',\n",
       " 'wide',\n",
       " 'straight',\n",
       " 'designer',\n",
       " 'burn',\n",
       " 'file',\n",
       " 'limit',\n",
       " 'list',\n",
       " 'google',\n",
       " 'agree',\n",
       " 'argument',\n",
       " 'murder',\n",
       " 'flag',\n",
       " 'simple',\n",
       " 'actual',\n",
       " 'make',\n",
       " 'hand',\n",
       " 'eye',\n",
       " 'tiny',\n",
       " 'suck',\n",
       " 'twice',\n",
       " '1',\n",
       " 'rarely',\n",
       " 'javascript',\n",
       " 'wall',\n",
       " 'mistake',\n",
       " 'perspective',\n",
       " 'allow',\n",
       " 'encourage',\n",
       " 'feature',\n",
       " 'andor',\n",
       " 'fine',\n",
       " 'forget',\n",
       " 'logic',\n",
       " 'internet',\n",
       " 'difference',\n",
       " 'especially',\n",
       " 'damage',\n",
       " 'security',\n",
       " 'organization',\n",
       " 'street',\n",
       " 'free',\n",
       " 'supply',\n",
       " 'group',\n",
       " 'display',\n",
       " 'young',\n",
       " '10',\n",
       " 'late']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate feature wordlist from comparison table.\n",
    "autogen_wordlist = comp.sort_values(by='diff', ascending=False).head(229)['word']\n",
    "list(autogen_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature wordlist without stop words.\n",
    "autogen_wordlist = ['fuck',\n",
    "                    'shit',\n",
    "                    'shitty',\n",
    "                    'people',\n",
    "                    'feel',\n",
    "                    'idiot',\n",
    "                    'bullshit',\n",
    "                    'stupid',\n",
    "                    'hell',\n",
    "                    'seriously',\n",
    "                    'right',\n",
    "                    'asshole',\n",
    "                    'crap',\n",
    "                    'web',\n",
    "                    'comment',\n",
    "                    'man',\n",
    "                    'different',\n",
    "                    'ugly',\n",
    "                    'care',\n",
    "                    'read',\n",
    "                    'damn',\n",
    "                    'time',\n",
    "                    'honest',\n",
    "                    'not',\n",
    "                    'yes',\n",
    " 'character',\n",
    " 'way',\n",
    " 'abuse',\n",
    " 'terrible',\n",
    " 'year',\n",
    " 'hate',\n",
    " 'begin',\n",
    " 'new',\n",
    " 'human',\n",
    " 'want',\n",
    " 'say',\n",
    " 'mean',\n",
    " 'interest',\n",
    " 'change',\n",
    " 'page',\n",
    " 'search',\n",
    " 'idiotic',\n",
    " 'website',\n",
    " 'datum',\n",
    " 'product',\n",
    " 'backwards',\n",
    " 'come',\n",
    " 'know',\n",
    " 'real',\n",
    " 'entire',\n",
    " 'god',\n",
    " 'case',\n",
    " 'prime',\n",
    " 'use',\n",
    " 'matt',\n",
    " 'criminal',\n",
    " 'buy',\n",
    " 'job',\n",
    " 'tell',\n",
    " 'fix',\n",
    " 'think',\n",
    " 'state',\n",
    " 'font',\n",
    " 'ui',\n",
    " 'googles',\n",
    " 'big',\n",
    " 'tv',\n",
    " 'black',\n",
    " 'error',\n",
    " 'help',\n",
    " 'call',\n",
    " 'complain',\n",
    " 'stuff',\n",
    " 'cost',\n",
    " 'apple',\n",
    " 'count',\n",
    " 'sorry',\n",
    " 'sure',\n",
    " 'user',\n",
    " 'support',\n",
    " 'app',\n",
    " 'death',\n",
    " 'edit',\n",
    " 'etc',\n",
    " 'problem',\n",
    " 'country',\n",
    " 'little',\n",
    " 'probably',\n",
    " 'deal',\n",
    " 'find',\n",
    " 'happen',\n",
    " 'microsoft',\n",
    " 'look',\n",
    " 'like',\n",
    " 'leave',\n",
    " 'language',\n",
    " 'idea',\n",
    " 'expect',\n",
    " 'understand',\n",
    " 'society',\n",
    " 'poor',\n",
    " 'provide',\n",
    " 'go',\n",
    " 'ask',\n",
    " 'amazon',\n",
    "  'pretty',\n",
    " 'target',\n",
    " 'include',\n",
    " 'stop',\n",
    " 'child',\n",
    " 'public',\n",
    " 'exactly',\n",
    " 'term',\n",
    " 'spit',\n",
    " 'try',\n",
    " 'issue',\n",
    " 'information',\n",
    " 'source',\n",
    " 'youth',\n",
    " 'safari',\n",
    " 'constructive',\n",
    " 'miss',\n",
    " 'skin',\n",
    " 'smell',\n",
    " 'cop',\n",
    " 'frustration',\n",
    " 'suppose',\n",
    " 'rest',\n",
    " 'attractive',\n",
    " 'similar',\n",
    " 'sense',\n",
    " 'discovery',\n",
    " 'business',\n",
    " 'assume',\n",
    " 'low',\n",
    " 'word',\n",
    " 'true',\n",
    " 'place',\n",
    " 'guy',\n",
    " 'lean',\n",
    " 'eg',\n",
    " 'reason',\n",
    " 'animal',\n",
    " 'transition',\n",
    " 'index',\n",
    " 'linus',\n",
    " 'small',\n",
    " 'shut',\n",
    " 'vw',\n",
    " 'campaign',\n",
    " 'css',\n",
    " 'guess',\n",
    " 'actually',\n",
    " 'override',\n",
    " 'render',\n",
    " 'cancer',\n",
    " 'require',\n",
    " 'mouse',\n",
    " 'cheat',\n",
    " 'to',\n",
    " 'early',\n",
    " 'mac',\n",
    " 'send',\n",
    " 'exclude',\n",
    " 'test',\n",
    " 'world',\n",
    " 'screw',\n",
    " 'car',\n",
    " 'have',\n",
    " 'service',\n",
    " 'arm',\n",
    " 'firefox',\n",
    " 'claim',\n",
    " 'defend',\n",
    " 'wide',\n",
    " 'straight',\n",
    " 'designer',\n",
    " 'burn',\n",
    " 'file',\n",
    " 'limit',\n",
    " 'list',\n",
    " 'google',\n",
    " 'agree',\n",
    " 'argument',\n",
    " 'murder',\n",
    " 'flag',\n",
    " 'simple',\n",
    " 'actual',\n",
    " 'make',\n",
    " 'hand',\n",
    " 'eye',\n",
    " 'tiny',\n",
    " 'suck',\n",
    " 'twice',\n",
    " 'rarely',\n",
    " 'javascript',\n",
    " 'wall',\n",
    " 'mistake',\n",
    " 'perspective',\n",
    " 'allow',\n",
    " 'encourage',\n",
    " 'feature',\n",
    " 'andor',\n",
    " 'fine',\n",
    " 'forget',\n",
    " 'logic',\n",
    " 'internet',\n",
    " 'difference',\n",
    " 'especially',\n",
    " 'damage',\n",
    " 'security',\n",
    " 'organization',\n",
    " 'street',\n",
    " 'free',\n",
    " 'supply',\n",
    " 'group',\n",
    " 'display',\n",
    " 'young',\n",
    " '10',\n",
    " 'late']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autogen_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_train = dtm_train_orig[autogen_wordlist]\n",
    "dtm_test = dtm_test_orig[autogen_wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(512, input_dim=220, activation='relu'))\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x219ea4c5708>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n",
    "model.fit(dtm_train, y_train_binary, epochs=25, \n",
    "          class_weight = {0: 0.010406, 1: 0.989594},\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2, \n",
    "          verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>7976.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.136391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.218928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.001173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.015079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.244935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.999579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  7976.000000\n",
       "mean      0.136391\n",
       "std       0.218928\n",
       "min       0.000000\n",
       "25%       0.001173\n",
       "50%       0.015079\n",
       "75%       0.244935\n",
       "max       0.999579"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions.\n",
    "nn_train_pred = model.predict(dtm_train)\n",
    "nn_test_pred = model.predict(dtm_test)\n",
    "pd.DataFrame(nn_train_pred).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics on training dataset.\n",
    "train_accuracy = accuracy_score(y_train_binary, np.rint(nn_train_pred))\n",
    "train_recall = recall_score(y_train_binary, np.rint(nn_train_pred))\n",
    "train_precision = precision_score(y_train_binary, np.rint(nn_train_pred))\n",
    "\n",
    "# Calculate metrics on test dataset.\n",
    "test_accuracy = accuracy_score(y_test_binary, np.rint(nn_test_pred))\n",
    "test_recall = recall_score(y_test_binary, np.rint(nn_test_pred))\n",
    "test_precision = precision_score(y_test_binary, np.rint(nn_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9274\n",
      "Train recall: 0.8313\n",
      "Train precision: 0.1088 \n",
      "\n",
      "Test accuracy: 0.9248\n",
      "Test recall: 0.1500\n",
      "Test precision: 0.0221\n"
     ]
    }
   ],
   "source": [
    "# Display metrics for training dataset.\n",
    "print(f'Train accuracy: {train_accuracy:.4f}')\n",
    "print(f'Train recall: {train_recall:.4f}')\n",
    "print(f'Train precision: {train_precision:.4f}', '\\n')\n",
    "\n",
    "# Display metrics for test dataset.\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "print(f'Test recall: {test_recall:.4f}')\n",
    "print(f'Test precision: {test_precision:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
